{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Graph Structured Data on Manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Learning Graph Structured Data (GSD) embeddings on manifolds received considerable attention recently. Many works showed the benefits\n",
    "of embedding GSD in hyperbolic space. In this trend, this tutorial shows how to learn GSD embedding in `geomstats` using the Poincaré Ball manifold. The method is applied\n",
    "to a practical and known GSD dataset, the Karate club. The latter and several other datasets are found in the `datasets.data` module."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![KarateEmbedding](figures/karate_embedding_iterations.gif \"segment\")\n",
    "*Learning Poincaré disk embedding of the Karate club graph dataset*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up\n",
    "\n",
    "\n",
    "We start by importing the logging module, to provide practical information throughout the embedding learning process.\n",
    "`matplotlib` and the `visualization` module will allow us to draw the embedding of the GSD on the manifold. The backend is imported\n",
    "and the `PoincareBall` manifold from the `geometry` module. A specific class for managing graph data is imported as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-6645734b3ebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpwd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pwd' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'pwd' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geomstats.backend as gs\n",
    "import geomstats.datasets.utils\n",
    "import geomstats.visualization as visualization\n",
    "from geomstats.datasets import graph_data_preparation as gdp\n",
    "from geomstats.geometry.poincare_ball import PoincareBall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Karate club is the well-known and much-used Zachary karate club network.\n",
    " The data was collected from the members of a university karate club by Wayne Zachary in 1977. \n",
    " Each node represents a member of the club, and each edge represents a tie between two members \n",
    " of the club. The graph is undirected. An often discussed problem using this dataset is to find the\n",
    "  two groups of people into which the karate club split after an argument between two teachers.\n",
    "Two variables are set as the path to the karate dataset (the graph adjacency matrix) and a second to the true \n",
    "labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "GRAPH_MATRIX_PATH = geomstats.datasets.utils.KARATE_PATH\n",
    "LABELS_PATH = geomstats.datasets.utils.KARATE_LABELS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Initialisation\n",
    "\n",
    "First of all, we define the main function and start by setting the following parameters needed for embedding:\n",
    "\n",
    "| Parameter | Description   |\n",
    "|------|------|\n",
    "|   random.seed  | An initial manually set number for generating pseudorandom numbers|\n",
    "| dim | Dimensions of the manifold used for embedding |\n",
    "|max_epochs|Number of iterations for learning embedding |\n",
    "|lr| Learning rate|\n",
    "|n_negative| Number of negative samples|\n",
    "|context_size| Size of the considered context for each node of the graph|\n",
    "\n",
    "Let us discuss a few things about the parameters of the above table.\n",
    "The number of dimensions should be high (i.e., 10+) for large datasets \n",
    "(i.e., where the number of nodes/edges is significantly large). \n",
    "In this tutorial we consider a dataset that is quite small with only 34 nodes. Therefore the Poincaré disk of only two dimensions is sufficient to\n",
    "capture the complexity of the graph and provide a faithful representation.\n",
    "Some parameters are hard to know in advance, such as `max_epochs` and `lr`, these should be tuned specifically per dataset.\n",
    "Visualizing can help with tuning the parameters or overmore, one can perform a grid search, to find values of these parameters \n",
    "maximising some performance. In learning embeddings, one can consider performance metrics such as\n",
    "a measure for clusters seperability or normalized mutual \n",
    "information (NMI) and so on.\n",
    "Similarly, the number of negative samples and context size can also be thought of as hyperparameters and will\n",
    "be further discussed in the sequel. An instance of the `Graph` class is created\n",
    " and set to the Karate club dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d8c71e195d0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcontext_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m karate_graph = gdp.Graph(graph_matrix_path=GRAPH_MATRIX_PATH,\n\u001b[1;32m----> 8\u001b[1;33m     labels_path=LABELS_PATH)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\geomstats_Hadi\\geomstats\\datasets\\graph_data_preparation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph_matrix_path, labels_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m                  labels_path=GRAPH_LABELS_PATH):\n\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_matrix_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0medges_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medges_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mlsp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'geomstats\\\\datasets\\\\data\\\\graph_karate/karate.txt'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'geomstats\\\\datasets\\\\data\\\\graph_karate/karate.txt'",
     "output_type": "error"
    }
   ],
   "source": [
    "gs.random.seed(1234)\n",
    "dim = 2\n",
    "max_epochs = 100\n",
    "lr = .05\n",
    "n_negative = 2\n",
    "context_size = 1\n",
    "karate_graph = gdp.Graph(graph_matrix_path=GRAPH_MATRIX_PATH,\n",
    "    labels_path=LABELS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some information on the dataset are displayed to provide insight on the dataset complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_vertices_by_edges =\\\n",
    "    [len(e_2) for _, e_2 in karate_graph.edges.items()]\n",
    "logging.info('Number of edges: %s', len(karate_graph.edges))\n",
    "logging.info(\n",
    "    'Mean vertices by edges: %s',\n",
    "    (sum(nb_vertices_by_edges, 0) / len(karate_graph.edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote $V$ as the set of nodes and $E \\subset V\\times V$ the set \n",
    "of edges. The goal of embedding GSD is to provide a faithful and exploitable representation \n",
    "of the graph structure. It is mainly achieved by preserving  *first-order* proximity \n",
    "that enforces nodes sharing edges to be close to each other. It can additionally \n",
    "preserve *second-order* proximity that enforces two nodes sharing the same context \n",
    "(i.e., nodes that are neighbours but not necessarily directly connected) to be close.\n",
    "Let $\\mathbb{B}^m$ be the Poincaré Ball of dimensions $m$ equipped with the distance function $d$.\n",
    "*first* and *second-order* proximities can be achieved by optimising the following loss functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function.\n",
    "\n",
    "To preserve first and second-order proximities we adopt a loss function similar to (add cite here) and consider the negative sampling approach:\n",
    "\n",
    "$$     \\mathcal{L} = - \\sum_{v_i\\in V} \\sum_{v_j \\in C_i} \\bigg[ log(\\sigma(-d^2(\\phi_i, \\phi_j'))) + \\sum_{v_k\\sim \\mathcal{P}_n} log(\\sigma(d^2(\\phi_i, \\phi_k')))  \\bigg]$$\n",
    "\n",
    "with $\\sigma(x)=\\frac{1}{1+e^{-x}}$ is the sigmoid function and $\\phi_i \\in \\mathbb{B}^m$ \n",
    "is the embedding of the $i$-th node of $V$, $C_i$ the nodes in the context of the \n",
    "$i$-th node, $\\phi_j'\\in \\mathbb{B}^m$ the embedding of $v_j\\in C_i$ and \n",
    "$\\mathcal{P}_n$ the negative sampling distribution over $V$: \n",
    "$\\mathcal{P}_n(v)=\\frac{deg(v)^{3/4}}{\\sum_{v_i\\in V}deg(v_i)^{3/4}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following formula to optimise $L$:\n",
    "\n",
    "$$ \\phi^{t+1} = \\text{Exp}_{\\phi^t} \\left( -lr \\frac{\\partial L}{\\partial \\phi} \\right) $$\n",
    "\n",
    "where $\\phi$ is a parameter of $L$, $t\\in\\{1,2,\\cdots\\}$ is the epoch iteration number \n",
    "and $lr$ is the learning rate. The formula consists of computing the usual gradient of the loss function\n",
    "which gives the direction in which the parameter should move. \n",
    "The Riemannian exponential map $\\text{Exp}$ is a function that takes a base point $\\phi^t$ and some \n",
    "direction vector $T$ and returns the point $\\phi^{t+1}$ such $\\phi^{t+1}$ belongs to the geodesic\n",
    "initiated from $\\phi{t}$ in the direction of $T$ and the length of the geoedesic curve between $\\phi^t$ and $\\phi^{t+1}$ is of length 1.\n",
    "\n",
    "Several steps are required to implement and optimise the loss function.\n",
    "Lets start first by defining the `log_sigmoid` corresponding as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_sigmoid(vector):\n",
    "    \"\"\"Logsigmoid function.\n",
    "\n",
    "    Apply log sigmoid function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vector : array-like, shape=[n_samples, dim]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : array-like, shape=[n_samples, dim]\n",
    "    \"\"\"\n",
    "    return gs.log((1 / (1 + gs.exp(-vector))))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To optimise $L$ we will further need the gradient of the sigmoid function implemented as:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grad_log_sigmoid(vector):\n",
    "    \"\"\"Gradient of log sigmoid function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vector : array-like, shape=[n_samples, dim]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradient : array-like, shape=[n_samples, dim]\n",
    "    \"\"\"\n",
    "    return 1 / (1 + gs.exp(vector))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Additionally the gradient of the squared distance is implemented as"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grad_squared_distance(point_a, point_b):\n",
    "    \"\"\"Gradient of squared hyperbolic distance.\n",
    "\n",
    "    Gradient of the squared distance based on the\n",
    "    Ball representation according to point_a\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point_a : array-like, shape=[n_samples, dim]\n",
    "        First point in hyperbolic space.\n",
    "    point_b : array-like, shape=[n_samples, dim]\n",
    "        Second point in hyperbolic space.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dist : array-like, shape=[n_samples, 1]\n",
    "        Geodesic squared distance between the two points.\n",
    "    \"\"\"\n",
    "    hyperbolic_metric = PoincareBall(2).metric\n",
    "    log_map = hyperbolic_metric.log(point_b, point_a)\n",
    "\n",
    "    return -2 * log_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture the structure of the graph, we proceed by computing the negatively sampled nodes and \n",
    "then generate random walks from each node. The latter is done via a special function within the `Graph` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "    negative_table_parameter = 5\n",
    "    negative_sampling_table = []\n",
    "\n",
    "    for i, nb_v in enumerate(nb_vertices_by_edges):\n",
    "        negative_sampling_table +=\\\n",
    "            ([i] * int((nb_v**(3. / 4.))) * negative_table_parameter)\n",
    "\n",
    "    negative_sampling_table = gs.array(negative_sampling_table)\n",
    "    random_walks = karate_graph.random_walk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize an array that will hold the dataset embedding with random points belonging to the Poincaré disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "    embeddings = gs.random.normal(size=(karate_graph.n_nodes, dim))\n",
    "    embeddings = embeddings * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then declare an instance of the `PoincareBall` manifold of two dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    hyperbolic_manifold = PoincareBall(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimising the loss function is performed numerically over the number of epochs. \n",
    "At each iteration, we will compute the gradient of $L$. Then the graph nodes are moved in the direction\n",
    "pointed by the gradient. The movement of the nodes is performed by following geodesics in the gradient direction.\n",
    " The key to obtain an embedding representing accurately the dataset, is to move the nodes smoothly rather\n",
    "than brutal movements. This is done by tuning the learning rate, such as at each epoch\n",
    "all the nodes made small movements."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    for epoch in range(max_epochs):\n",
    "        total_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember that we pre-computed a random walk from each node of the graph. Looking at the random walks,\n",
    " notice that nodes having many edges appear more often. Nodes appearing more often than others will therefore\n",
    " have more embedding updates. This is one of the main reasons why random walks have proven to be effective\n",
    " for capturing the graph structure and use nodes along these walks for optimizing $L$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        for path in random_walks:\n",
    "\n",
    "            for example_index, one_path in enumerate(path):\n",
    "                context_index = path[max(0, example_index - context_size):\n",
    "                                     min(example_index + context_size,\n",
    "                                     len(path))]\n",
    "                negative_index =\\\n",
    "                    gs.random.randint(negative_sampling_table.shape[0],\n",
    "                                      size=(len(context_index),\n",
    "                                      n_negative))\n",
    "                negative_index = negative_sampling_table[negative_index]\n",
    "\n",
    "                example_embedding = embeddings[one_path]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given the embedding of each node, context and negative sample, we compute the gradient of the loss function\n",
    "for the specific node and the value of the loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "                for one_context_i, one_negative_i in zip(context_index,\n",
    "                                                         negative_index):\n",
    "                    context_embedding = embeddings[one_context_i]\n",
    "                    negative_embedding = embeddings[one_negative_i]\n",
    "                    l, g_ex = loss(\n",
    "                        example_embedding,\n",
    "                        context_embedding,\n",
    "                        negative_embedding,\n",
    "                        hyperbolic_manifold)\n",
    "                    total_loss.append(l)\n",
    "\n",
    "                    example_to_update = embeddings[one_path]\n",
    "                    embeddings[one_path] = hyperbolic_manifold.metric.exp(\n",
    "                        -lr * g_ex, example_to_update)\n",
    "\n",
    "        logging.info(\n",
    "            'iteration %d loss_value %f',\n",
    "            epoch, sum(total_loss, 0) / len(total_loss))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting results\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    colors = {1: 'b', 2: 'r'}\n",
    "    circle = visualization.PoincareDisk(point_type='ball')\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    circle.add_points(gs.array([[0, 0]]))\n",
    "    circle.set_ax(ax)\n",
    "    circle.draw(ax=ax)\n",
    "    for i_embedding, embedding in enumerate(embeddings):\n",
    "        plt.scatter(\n",
    "            embedding[0], embedding[1],\n",
    "            c=colors[karate_graph.labels[i_embedding][0]])\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss(example_embedding, context_embedding, negative_embedding,\n",
    "         manifold):\n",
    "    \"\"\"Compute loss and grad.\n",
    "\n",
    "    Compute loss and grad given embedding of the current example,\n",
    "    embedding of the context and negative sampling embedding.\n",
    "    \"\"\"\n",
    "    n_edges, dim =\\\n",
    "        negative_embedding.shape[0], example_embedding.shape[-1]\n",
    "    example_embedding = gs.expand_dims(example_embedding, 0)\n",
    "    context_embedding = gs.expand_dims(context_embedding, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compute the distance between each embedded node and its context\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    positive_distance =\\\n",
    "        manifold.metric.squared_dist(\n",
    "            example_embedding, context_embedding)\n",
    "    positive_loss =\\\n",
    "        log_sigmoid(-positive_distance)\n",
    "\n",
    "    reshaped_example_embedding =\\\n",
    "        gs.repeat(example_embedding, n_edges, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, compute the squared distances between embedding of each node and its negative samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    negative_distance =\\\n",
    "        manifold.metric.squared_dist(\n",
    "            reshaped_example_embedding, negative_embedding)\n",
    "    negative_loss = log_sigmoid(negative_distance)\n",
    "\n",
    "    total_loss = -(positive_loss + negative_loss.sum())\n",
    "\n",
    "    positive_log_sigmoid_grad =\\\n",
    "        -grad_log_sigmoid(-positive_distance)\n",
    "\n",
    "    positive_distance_grad =\\\n",
    "        grad_squared_distance(example_embedding, context_embedding)\n",
    "\n",
    "    positive_grad =\\\n",
    "        gs.repeat(positive_log_sigmoid_grad, dim, axis=-1)\\\n",
    "        * positive_distance_grad\n",
    "\n",
    "    negative_distance_grad =\\\n",
    "        grad_squared_distance(reshaped_example_embedding, negative_embedding)\n",
    "\n",
    "    negative_distance = gs.to_ndarray(negative_distance,\n",
    "                                      to_ndim=2, axis=-1)\n",
    "    negative_log_sigmoid_grad =\\\n",
    "        grad_log_sigmoid(negative_distance)\n",
    "\n",
    "    negative_grad = negative_log_sigmoid_grad\\\n",
    "        * negative_distance_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally the gradient of $L$ at the current node embedding is obtained and returned as"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    example_grad = -(positive_grad + negative_grad.sum(axis=0))\n",
    "\n",
    "    return total_loss, example_grad\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"cite2c-biblio\"></div>"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "7875465/KRA9K53S": {
     "author": [
      {
       "family": "Nickel",
       "given": "Maximillian"
      },
      {
       "family": "Kiela",
       "given": "Douwe"
      }
     ],
     "container-title": "Advances in Neural Information Processing Systems 30 (NIPS)",
     "id": "7875465/KRA9K53S",
     "issued": {
      "year": 2017
     },
     "page": "6338–6347",
     "page-first": "6338",
     "publisher": "Curran Associates, Inc.",
     "title": "Poincaré Embeddings for Learning Hierarchical Representations",
     "type": "chapter"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}