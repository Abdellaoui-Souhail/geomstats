{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Graph Structured Data on Manifolds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Recently, the embedding of Graph Structured Data (GSD) on manifolds has received considerable attention. In particular, much work has shown that hyperbolic spaces are beneficial for a wide variety of tasks with GSD. This tutorial shows how to learn such embeddings in `geomstats` using the Poincaré Ball manifold using the well-known 'Karate Club' dataset. This and several others can be found in the `datasets.data` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![KarateEmbedding](figures/karate_embedding_iterations.gif \"segment\")\n",
    "*Learning a Poincaré disk embedding of the Karate club graph dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "\n",
    "We start by importing standard tools for logging and visualization, allowing us to draw the embedding of the GSD on the manifold. Next, we import the manifold of interest, visualization tools, and other methods from `geomstats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geomstats\n",
    "import geomstats.backend as gs\n",
    "import geomstats.visualization as visualization\n",
    "from geomstats.datasets import graph_data_preparation as gdp\n",
    "from geomstats.geometry.poincare_ball import PoincareBall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Zachary karate club network was collected from the members of a university karate club by Wayne Zachary in 1977. Each node represents a member of the club, and each edge represents an undirected relation between two members. An often discussed problem using this dataset is to find the two groups of people into which the karate club split after an argument between two teachers.\n",
    "    \n",
    "Two variables are set here: the path to the graph adjacency matrix and the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# why do we do this and not just load the dataset for them in one function within geomstats.datasets.utils?\n",
    "GRAPH_MATRIX_PATH = geomstats.datasets.utils.KARATE_PATH \n",
    "LABELS_PATH = geomstats.datasets.utils.KARATE_LABELS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Initialization\n",
    "\n",
    "We define the following parameters needed for embedding:\n",
    "\n",
    "| Parameter | Description   |\n",
    "|:------|:------|\n",
    "|   random.seed  | An initial manually set number for generating pseudorandom numbers|\n",
    "| dim | Dimensions of the manifold used for embedding |\n",
    "|max_epochs|Number of iterations for learning the embedding |\n",
    "|lr| Learning rate|\n",
    "|n_negative| Number of negative samples|\n",
    "|context_size| Size of the considered context for each node of the graph|\n",
    "\n",
    "Let us discuss a few things about the parameters of the above table.\n",
    "The number of dimensions should be high (i.e., 10+) for large datasets \n",
    "(i.e., where the number of nodes/edges is significantly large). \n",
    "In this tutorial we consider a dataset that is quite small with only 34 nodes. Therefore the Poincaré disk of only two dimensions is sufficient to\n",
    "capture the complexity of the graph and provide a faithful representation.\n",
    "Some parameters are hard to know in advance, such as `max_epochs` and `lr`. These should be tuned specifically for each dataset.\n",
    "Visualization can help with tuning the parameters. Also, one can perform a grid search to find values of these parameters which maximize some performance function. In learning embeddings, one can consider performance metrics such as\n",
    "a measure for cluster seperability or normalized mutual \n",
    "information (NMI) or others.\n",
    "Similarly, the number of negative samples and context size can also be thought of as hyperparameters and will\n",
    "be further discussed in the sequel. An instance of the `Graph` class is created\n",
    " and set to the Karate club dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d8c71e195d0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcontext_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m karate_graph = gdp.Graph(graph_matrix_path=GRAPH_MATRIX_PATH,\n\u001b[1;32m----> 8\u001b[1;33m     labels_path=LABELS_PATH)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\geomstats_Hadi\\geomstats\\datasets\\graph_data_preparation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph_matrix_path, labels_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m                  labels_path=GRAPH_LABELS_PATH):\n\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_matrix_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0medges_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medges_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mlsp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'geomstats\\\\datasets\\\\data\\\\graph_karate/karate.txt'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'geomstats\\\\datasets\\\\data\\\\graph_karate/karate.txt'",
     "output_type": "error"
    }
   ],
   "source": [
    "gs.random.seed(1234)\n",
    "dim = 2\n",
    "max_epochs = 100\n",
    "lr = .05\n",
    "n_negative = 2\n",
    "context_size = 1\n",
    "karate_graph = gdp.Graph(graph_matrix_path=GRAPH_MATRIX_PATH,\n",
    "    labels_path=LABELS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some information abbout the dataset is displayed to provide insight into its complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_vertices_by_edges =\\\n",
    "    [len(e_2) for _, e_2 in karate_graph.edges.items()]\n",
    "logging.info('Number of edges: %s', len(karate_graph.edges))\n",
    "logging.info(\n",
    "    'Mean vertices by edges: %s',\n",
    "    (sum(nb_vertices_by_edges, 0) / len(karate_graph.edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote $V$ as the set of nodes and $E \\subset V\\times V$ the set \n",
    "of edges. The goal of embedding GSD is to provide a faithful and exploitable representation \n",
    "of the graph structure. It is mainly achieved by preserving  *first-order* proximity \n",
    "that enforces nodes sharing edges to be close to each other. It can additionally \n",
    "preserve *second-order* proximity that enforces two nodes sharing the same context \n",
    "(i.e., nodes that are neighbours but not necessarily directly connected) to be close.\n",
    "Let $\\mathbb{B}^m$ be the Poincaré Ball of dimension $m$ equipped with the distance function $d$.\n",
    "*first* and *second-order* proximities can be achieved by optimising the following loss functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function.\n",
    "\n",
    "To preserve first and second-order proximities we adopt a loss function similar to <cite data-cite=\"7875465/KRA9K53S\"></cite> and consider the negative sampling approach as in <cite data-cite=\"7875465/9CMMEH2F\"></cite> :\n",
    "\n",
    "$$     \\mathcal{L} = - \\sum_{v_i\\in V} \\sum_{v_j \\in C_i} \\bigg[ log(\\sigma(-d^2(\\phi_i, \\phi_j'))) + \\sum_{v_k\\sim \\mathcal{P}_n} log(\\sigma(d^2(\\phi_i, \\phi_k')))  \\bigg]$$\n",
    "\n",
    "where $\\sigma(x)=\\frac{1}{1+e^{-x}}$ is the sigmoid function and $\\phi_i \\in \\mathbb{B}^m$ \n",
    "is the embedding of the $i$-th node of $V$, $C_i$ the nodes in the context of the \n",
    "$i$-th node, $\\phi_j'\\in \\mathbb{B}^m$ the embedding of $v_j\\in C_i$ and \n",
    "$\\mathcal{P}_n$ the negative sampling distribution over $V$: \n",
    "$\\mathcal{P}_n(v)=\\frac{deg(v)^{3/4}}{\\sum_{v_i\\in V}deg(v_i)^{3/4}}$. \n",
    "Intuitively one can see that to minimizing $L$, the distance between $v_i$ and $v_j$ should get smaller, while the one between\n",
    "$v_i$ and $v_k$ would get larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riemannian optimization.\n",
    "We will use the following formula to optimise $L$:\n",
    "\n",
    "$$ \\phi^{t+1} = \\text{Exp}_{\\phi^t} \\left( -lr \\frac{\\partial L}{\\partial \\phi} \\right) $$\n",
    "\n",
    "where $\\phi$ is a parameter of $L$, $t\\in\\{1,2,\\cdots\\}$ is the epoch iteration number \n",
    "and $lr$ is the learning rate. The formula consists of computing the usual gradient of the loss function\n",
    "which gives the direction in which the parameter should move. \n",
    "The Riemannian exponential map $\\text{Exp}$ is a function that takes a base point $\\phi^t$ and some \n",
    "direction vector $T$ and returns the point $\\phi^{t+1}$ such that $\\phi^{t+1}$ belongs to the geodesic\n",
    "initiated from $\\phi{t}$ in the direction of $T$ and the length of the geoedesic curve between $\\phi^t$ and $\\phi^{t+1}$ is of 1 unit.\n",
    "The Riemannian exponential map is implemented as a method of the `PoincareBallMetric` class in the `geometry` module of `geomstats`.\n",
    "\n",
    "Therefore to minimize $L$ we will need to compute its gradient. Several steps are required to do so,\n",
    "1. Compute the gradient of the squared distance\n",
    "2. Compute the gradient of the log sigmoid\n",
    "3. Compute the gradient of the composision of 1. and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1., we use the formula proposed by <cite data-cite=\"7875465/TLSACAEQ\"></cite> which uses the Riemannian logarithmic map to compute the gradient of the distance. \n",
    "This is implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def grad_squared_distance(point_a, point_b):\n",
    "    \"\"\"Gradient of squared hyperbolic distance.\n",
    "\n",
    "    Gradient of the squared distance based on the\n",
    "    Ball representation according to point_a\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point_a : array-like, shape=[n_samples, dim]\n",
    "        First point in hyperbolic space.\n",
    "    point_b : array-like, shape=[n_samples, dim]\n",
    "        Second point in hyperbolic space.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dist : array-like, shape=[n_samples, 1]\n",
    "        Geodesic squared distance between the two points.\n",
    "    \"\"\"\n",
    "    hyperbolic_metric = PoincareBall(2).metric\n",
    "    log_map = hyperbolic_metric.log(point_b, point_a)\n",
    "\n",
    "    return -2 * log_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For 2. define the `log_sigmoid` corresponding as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_sigmoid(vector):\n",
    "    \"\"\"Logsigmoid function.\n",
    "\n",
    "    Apply log sigmoid function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vector : array-like, shape=[n_samples, dim]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : array-like, shape=[n_samples, dim]\n",
    "    \"\"\"\n",
    "    return gs.log((1 / (1 + gs.exp(-vector))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The gradient of the logarithm of sigmoid function is implemented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "def grad_log_sigmoid(vector):\n",
    "    \"\"\"Gradient of log sigmoid function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vector : array-like, shape=[n_samples, dim]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gradient : array-like, shape=[n_samples, dim]\n",
    "    \"\"\"\n",
    "    return 1 / (1 + gs.exp(vector))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing the graph structure\n",
    "At this point we have the necessary bricks to compute the resulting gradient of $L$. We are ready to prepare\n",
    "the nodes $v_i$, $v_j$ and $v_k$ and their embeddings $\\phi_i$, $\\phi'_j$ and $\\phi'_k$.\n",
    "First, initialize an array that will hold embeddings $\\phi_i$ of each node $v_i\\in V$ with random points belonging to the Poincaré disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    embeddings = gs.random.normal(size=(karate_graph.n_nodes, dim))\n",
    "    embeddings = embeddings * 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, to prepare the context nodes $v_j$ for each node $v_i$, we compute random walks initialised from each $v_i$ \n",
    "up to some length (5 by default). The latter is done via a special function within the `Graph` class.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "    random_walks = karate_graph.random_walk()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Negatively sampled nodes $v_k$ are chosen according to the previously defined probability distribution function\n",
    "$\\mathcal{P}_n(v_k)$ implemented as"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    negative_table_parameter = 5\n",
    "    negative_sampling_table = []\n",
    "\n",
    "    for i, nb_v in enumerate(nb_vertices_by_edges):\n",
    "        negative_sampling_table +=\\\n",
    "            ([i] * int((nb_v**(3. / 4.))) * negative_table_parameter)\n",
    "\n",
    "    negative_sampling_table = gs.array(negative_sampling_table)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Numerically optimizing the loss function\n",
    "Optimising the loss function is performed numerically over the number of epochs. \n",
    "At each iteration, we will compute the gradient of $L$. Then the graph nodes are moved in the direction\n",
    "pointed by the gradient. The movement of the nodes is performed by following geodesics in the gradient direction.\n",
    " The key to obtain an embedding representing accurately the dataset, is to move the nodes smoothly rather\n",
    "than brutal movements. This is done by tuning the learning rate, such as at each epoch\n",
    "all the nodes made small movements."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Declare an instance of the `PoincareBall` manifold of two dimensions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    hyperbolic_manifold = PoincareBall(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This first level loop iterates over the epochs, the table `total_loss` will record the value of $L$ at each iteration\n",
    "and help us track the minimization of $L$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    for epoch in range(max_epochs):\n",
    "        total_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remember that we pre-computed random walks from each node of the graph. Observing these walks, notice that nodes having \n",
    "many edges appear more often. Nodes appearing more often than others in the walks\n",
    "can be considered as important crossroads and will therefore be subject to a greater number of embedding updates. \n",
    "This is one of the main reasons why random walks have proven to be effective\n",
    "in capturing the structure of graphs.\n",
    "A second level loop iterates over each path in the walks and a third loop on each \n",
    "node along each path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "        for path in random_walks:\n",
    "\n",
    "            for example_index, one_path in enumerate(path):"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The context of each $v_i$ will be the set of nodes $v_j$ belonging \n",
    "to a window of the random walk from $v_i$ of size `context_size` specified earlier. Similarly, we use\n",
    "the same `context_size` to choose the number of negative samples."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "                context_index = path[max(0, example_index - context_size):\n",
    "                                     min(example_index + context_size,\n",
    "                                     len(path))]\n",
    "                negative_index =\\\n",
    "                    gs.random.randint(negative_sampling_table.shape[0],\n",
    "                                      size=(len(context_index),\n",
    "                                      n_negative))\n",
    "                negative_index = negative_sampling_table[negative_index]\n",
    "\n",
    "                example_embedding = embeddings[one_path]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Given some $v_i$, $v_j$ and $v_k$ we compute the total loss function and its gradient \n",
    "and then update the new value of $phi_i$ via the Riemannian exponential map as specified earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "                for one_context_i, one_negative_i in zip(context_index,\n",
    "                                                         negative_index):\n",
    "                    context_embedding = embeddings[one_context_i]\n",
    "                    negative_embedding = embeddings[one_negative_i]\n",
    "                    l, g_ex = loss(\n",
    "                        example_embedding,\n",
    "                        context_embedding,\n",
    "                        negative_embedding,\n",
    "                        hyperbolic_manifold)\n",
    "                    total_loss.append(l)\n",
    "\n",
    "                    example_to_update = embeddings[one_path]\n",
    "                    embeddings[one_path] = hyperbolic_manifold.metric.exp(\n",
    "                        -lr * g_ex, example_to_update)\n",
    "\n",
    "        logging.info(\n",
    "            'iteration %d loss_value %f',\n",
    "            epoch, sum(total_loss, 0) / len(total_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting results\n",
    "Once the the max number of epochs is achieved, we can plot the resulting `embeddings` array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    colors = {1: 'b', 2: 'r'}\n",
    "    circle = visualization.PoincareDisk(point_type='ball')\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    circle.add_points(gs.array([[0, 0]]))\n",
    "    circle.set_ax(ax)\n",
    "    circle.draw(ax=ax)\n",
    "    for i_embedding, embedding in enumerate(embeddings):\n",
    "        plt.scatter(\n",
    "            embedding[0], embedding[1],\n",
    "            c=colors[karate_graph.labels[i_embedding][0]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss(example_embedding, context_embedding, negative_embedding,\n",
    "         manifold):\n",
    "    \"\"\"Compute loss and grad.\n",
    "\n",
    "    Compute loss and grad given embedding of the current example,\n",
    "    embedding of the context and negative sampling embedding.\n",
    "    \"\"\"\n",
    "    n_edges, dim =\\\n",
    "        negative_embedding.shape[0], example_embedding.shape[-1]\n",
    "    example_embedding = gs.expand_dims(example_embedding, 0)\n",
    "    context_embedding = gs.expand_dims(context_embedding, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We compute the distance between each embedded node and its context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    positive_distance =\\\n",
    "        manifold.metric.squared_dist(\n",
    "            example_embedding, context_embedding)\n",
    "    positive_loss =\\\n",
    "        log_sigmoid(-positive_distance)\n",
    "\n",
    "    reshaped_example_embedding =\\\n",
    "        gs.repeat(example_embedding, n_edges, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similarly, compute the squared distances between embedding of each node and its negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    negative_distance =\\\n",
    "        manifold.metric.squared_dist(\n",
    "            reshaped_example_embedding, negative_embedding)\n",
    "    negative_loss = log_sigmoid(negative_distance)\n",
    "\n",
    "    total_loss = -(positive_loss + negative_loss.sum())\n",
    "\n",
    "    positive_log_sigmoid_grad =\\\n",
    "        -grad_log_sigmoid(-positive_distance)\n",
    "\n",
    "    positive_distance_grad =\\\n",
    "        grad_squared_distance(example_embedding, context_embedding)\n",
    "\n",
    "    positive_grad =\\\n",
    "        gs.repeat(positive_log_sigmoid_grad, dim, axis=-1)\\\n",
    "        * positive_distance_grad\n",
    "\n",
    "    negative_distance_grad =\\\n",
    "        grad_squared_distance(reshaped_example_embedding, negative_embedding)\n",
    "\n",
    "    negative_distance = gs.to_ndarray(negative_distance,\n",
    "                                      to_ndim=2, axis=-1)\n",
    "    negative_log_sigmoid_grad =\\\n",
    "        grad_log_sigmoid(negative_distance)\n",
    "\n",
    "    negative_grad = negative_log_sigmoid_grad\\\n",
    "        * negative_distance_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally the gradient of $L$ at the current node embedding is obtained and returned as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    example_grad = -(positive_grad + negative_grad.sum(axis=0))\n",
    "\n",
    "    return total_loss, example_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "<div class=\"cite2c-biblio\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "7875465/9CMMEH2F": {
     "author": [
      {
       "family": "Mikolov",
       "given": "Tomas"
      },
      {
       "family": "Sutskever",
       "given": "Ilya"
      },
      {
       "family": "Chen",
       "given": "Kai"
      },
      {
       "family": "Corrado",
       "given": "Greg S"
      },
      {
       "family": "Dean",
       "given": "Jeff"
      }
     ],
     "container-title": "Advances in Neural Information Processing Systems 26 (NIPS)",
     "id": "7875465/9CMMEH2F",
     "issued": {
      "year": 2013
     },
     "page": "3111–3119",
     "page-first": "3111",
     "publisher": "Curran Associates, Inc.",
     "title": "Distributed Representations of Words and Phrases and their Compositionality",
     "type": "chapter"
    },
    "7875465/KRA9K53S": {
     "author": [
      {
       "family": "Nickel",
       "given": "Maximillian"
      },
      {
       "family": "Kiela",
       "given": "Douwe"
      }
     ],
     "container-title": "Advances in Neural Information Processing Systems 30 (NIPS)",
     "id": "7875465/KRA9K53S",
     "issued": {
      "year": 2017
     },
     "page": "6338–6347",
     "page-first": "6338",
     "publisher": "Curran Associates, Inc.",
     "title": "Poincaré Embeddings for Learning Hierarchical Representations",
     "type": "chapter"
    },
    "7875465/TLSACAEQ": {
     "author": [
      {
       "family": "Arnaudon",
       "given": "Marc"
      },
      {
       "family": "Barbaresco",
       "given": "Frédéric"
      },
      {
       "family": "Yang",
       "given": "Le"
      }
     ],
     "container-title": "Journal of Selected Topics in Signal Processing",
     "id": "7875465/TLSACAEQ",
     "issue": "4",
     "issued": {
      "year": 2013
     },
     "page": "595–604",
     "page-first": "595",
     "title": "Riemannian Medians and Means With Applications to Radar Signal Processing",
     "type": "article-journal",
     "volume": "7"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}